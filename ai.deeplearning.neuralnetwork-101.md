---
id: dycmeqk6ramhcw8cir1u5g5
title: Neuralnetwork 101
desc: ''
updated: 1756135138870
created: 1756135125104
---

# ğŸ§  Neural Networks 101

A neural network is basically a **stack of function approximators**. At its core is the **artificial neuron**, inspired by the human brain.

---

## 1ï¸âƒ£ The Artificial Neuron

* Inputs: $x_1, x_2, ..., x_n$
* Weights: $w_1, w_2, ..., w_n$ â†’ tells importance of each input
* Bias: $b$ â†’ allows shifting activation
* Activation function: $f(\cdot)$ â†’ introduces non-linearity

**Equation:**

$$
y = f(w_1 x_1 + w_2 x_2 + ... + w_n x_n + b)
$$

ğŸ’¡ Intuition:

* Think of the neuron as **â€œweighted votingâ€** of its inputs.
* Activation = â€œshould I fire or not?â€

---

## 2ï¸âƒ£ Activation Functions

Why not just use linear outputs?

* Without non-linearity â†’ network collapses into a single linear function, no matter how deep.

Common activations:

| Function   | Equation                                | Intuition                                |
| ---------- | --------------------------------------- | ---------------------------------------- |
| Sigmoid    | $1 / (1 + e^{-x})$                      | Squashes to \[0,1], probabilistic output |
| Tanh       | $(e^x - e^{-x}) / (e^x + e^{-x})$       | \[-1,1], zero-centered                   |
| ReLU       | $max(0, x)$                             | Sparse activation, faster training       |
| Leaky ReLU | $x \text{ if } x>0 \text{ else } 0.01x$ | Avoids dead neurons                      |

ğŸ’¡ Rule of thumb:

* ReLU for hidden layers, Sigmoid for output if binary classification.

---

## 3ï¸âƒ£ Forward Pass (Prediction)

1. Inputs â†’ each neuron multiplies inputs by weights, adds bias.
2. Pass through activation â†’ neuron â€œfiresâ€
3. Outputs â†’ feed to next layer
4. Repeat â†’ until final layer produces predictions

ğŸ’¡ Intuition:

* Forward pass = **how the network transforms inputs into outputs**.

---

## 4ï¸âƒ£ Loss Function

* Measures how far predictions $\hat{y}$ are from true labels $y$
* Common choices:

  * Regression â†’ MSE: $(y - \hat{y})^2$
  * Classification â†’ Cross-entropy

---

## 5ï¸âƒ£ Backpropagation (Learning)

Goal: **update weights to reduce loss**

### Step 1: Compute Gradient

* Derivative of loss w\.r.t each weight â†’ tells how changing that weight affects the loss

$$
\frac{\partial L}{\partial w} = ?
$$

* Done using **chain rule** from calculus

---

### Step 2: Weight Update (Gradient Descent)

$$
w := w - \eta \frac{\partial L}{\partial w}
$$

* $\eta$ = learning rate (step size)
* Repeat over all weights and all layers

ğŸ’¡ Intuition:

* Backprop = **blame assignment**

  * Which weights contributed to the error?
  * Adjust them slightly â†’ next prediction is better

---

### Step 3: Iteration (Epochs)

* Forward pass â†’ compute loss
* Backprop â†’ adjust weights
* Repeat for many epochs until loss converges

---

## 6ï¸âƒ£ Layers = Network Depth

* **Input layer** â†’ raw features
* **Hidden layers** â†’ feature extraction / representation learning
* **Output layer** â†’ predictions

ğŸ’¡ Deeper networks = can learn **hierarchical features**

* Shallow â†’ simple patterns
* Deep â†’ complex patterns (edges â†’ shapes â†’ objects in images)

---

## 7ï¸âƒ£ Intuition Example

Imagine predicting house price:

1. Input: size, location, bedrooms
2. Hidden layer: neurons detect patterns (e.g., â€œluxury locationâ€ or â€œbig sizeâ€)
3. Next layer: combine features â†’ â€œlikely high priceâ€
4. Output: predicted price

* Network â€œautomatically learnsâ€ useful combinations of features instead of manual feature engineering

---

## âš¡ TL;DR â€“ Neural Network Flow

1. **Neuron**: weighted sum + activation
2. **Forward Pass**: input â†’ hidden layers â†’ output
3. **Compute Loss**: compare prediction vs truth
4. **Backprop**: compute gradients â†’ adjust weights
5. **Iterate**: until predictions are good

---

ğŸ’¡ **Rule of Thumb for Remembering Neural Nets:**

> â€œForward = prediction, Backward = blame & adjust, Repeat until smart.â€

---

