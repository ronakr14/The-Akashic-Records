---
id: ht7mh1pahcx7hb84fedb7t3
title: Principalcomponentanalysis
desc: ''
updated: 1756134629018
created: 1756134621823
---

# ğŸ”» PCA (Principal Component Analysis) â€“ Math & Geometry Intuition

---

## ğŸ”¹ The Problem

High-dimensional data is messy:

* Think of a dataset with 100 features (columns).
* Some are redundant (highly correlated).
* Some add noise.
* We want to **reduce dimensions** but still keep most of the â€œinformation.â€

---

## ğŸ”¹ The Trick

PCA finds **new axes (directions) in feature space** that:

1. Capture the maximum variance (spread of data).
2. Are orthogonal (uncorrelated).

ğŸ‘‰ Instead of describing data with old features, we describe it with **Principal Components** (PCs).

---

## ğŸ”¹ Step 1: Center the Data

Subtract the mean of each feature â†’ move dataset to origin.

* Important because PCA cares about variance, not absolute values.

---

## ğŸ”¹ Step 2: Find Covariance Matrix

Covariance tells us how features move together.

$$
Cov(X) = \frac{1}{n-1} X^T X
$$

* Large covariance between two features = they carry redundant info.
* Example: â€œHeightâ€ and â€œArm Lengthâ€ â†’ highly correlated.

---

## ğŸ”¹ Step 3: Eigenvectors & Eigenvalues

This is where linear algebra enters like a rockstar ğŸ¸.

* Eigenvectors = **directions** of maximum variance.
* Eigenvalues = **how much variance** is captured in that direction.

We sort eigenvectors by eigenvalues:

* PC1 (1st component) â†’ captures most variance.
* PC2 â†’ next most variance, orthogonal to PC1.
* â€¦ and so on.

---

## ğŸ”¹ Step 4: Project Data

We take original data and project it onto top k eigenvectors.

$$
Z = X W_k
$$

Where:

* $X$ = centered data.
* $W_k$ = top k eigenvectors.
* $Z$ = reduced representation.

---

## ğŸ”¹ Intuition with Example

Imagine a 2D dataset (points scattered in an elongated ellipse).

* The biggest spread is along the ellipseâ€™s major axis.
* PCA rotates the axes â†’ aligns PC1 with that major axis, PC2 with the minor axis.
* If PC2 doesnâ€™t carry much variance, we can drop it â†’ dataset reduced from 2D â†’ 1D while still preserving structure.

ğŸ‘‰ In higher dimensions, PCA does the same but with linear algebra instead of eyeballs.

---

## ğŸ”¹ Variance Retained

We choose number of components k by checking **explained variance ratio**:

* Example: PC1 explains 70%, PC2 explains 20%, PC3 explains 5%.
* Using just first 2 PCs â†’ 90% variance retained.

---

# ğŸ† Real-World Applications of PCA

* **Data Compression** â†’ reduce features without losing much information.
* **Visualization** â†’ reducing 100D word embeddings into 2D for plotting.
* **Noise Filtering** â†’ small-variance PCs often capture noise.
* **Finance** â†’ analyze correlated stocks by reducing them into a few PCs (market vs. sector trends).
* **Image Compression** â†’ represent images with fewer components.

---

# âš¡ Quick Visual Analogy

* Imagine 100 CCTV cameras filming the same event from slightly different angles.
* PCA says: â€œMost of whatâ€™s happening can be captured by just 2â€“3 best cameras (directions). Letâ€™s drop the redundant ones.â€

---

# ğŸ”‘ Related Dimensionality Reduction Methods

* **t-SNE / UMAP** â†’ non-linear, great for visualization (cluster discovery).
* **Autoencoders (Deep Learning)** â†’ neural nets learn compressed representations.
* PCA is linear & algebraic, t-SNE/UMAP are more about non-linear manifolds.

---

