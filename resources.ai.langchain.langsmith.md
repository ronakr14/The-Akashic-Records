---
id: qfpcvapc2vp11g0rzr2fxjg
title: Langsmith
desc: ''
updated: 1753517715371
created: 1753517710277
---
tags: [master, langsmith, langchain, debugging, evaluation, llmops, agentic-workflows]

---

## ğŸ“Œ Topic Overview

**LangSmith** is an **LLMOps platform** by LangChain that helps you **debug, trace, evaluate, and monitor** LLM-based applications â€” especially those built with LangChain.

Where LangChain is your â€œframeworkâ€ and Langflow your â€œUI,â€ **LangSmith is your black box recorder and QA lab.** It shows how your agents, tools, prompts, and chains behave across inputs, in real-time or retrospectively.

> ğŸ§  Think of LangSmith as a **New Relic + Postman + GitHub Copilot** for LLM pipelines.

LangSmith helps you:
- Visualize execution traces across chains/tools/agents
- Evaluate outputs via human or LLM feedback
- Log and version your experiments
- Monitor production workflows
- Compare prompts and model performance over time

---

## ğŸš€ 80/20 Roadmap

| Stage | Concept                  | Why It Matters                                               |
|-------|--------------------------|---------------------------------------------------------------|
| 1ï¸âƒ£    | Project & Run Tracking    | Centralized logging for LangChain chains                     |
| 2ï¸âƒ£    | Execution Tracing         | Visual debugger for complex chains and agent tool calls      |
| 3ï¸âƒ£    | Dataset Logging & Versioning | Input/output capture for evaluation and CI/CD            |
| 4ï¸âƒ£    | Evaluation + Feedback     | Auto/Manual scoring to judge model responses                 |
| 5ï¸âƒ£    | Prompt Testing & Comparison | A/B testing and regression of prompt chains                |
| 6ï¸âƒ£    | LLM Tracing & Token Logs  | Monitor latency, token usage, and model response times       |
| 7ï¸âƒ£    | Production Monitoring     | Alerts, dashboards, and tracing for live apps                |

---

## ğŸ› ï¸ Practical Tasks

- âœ… Create a LangSmith account and get an API key  
- âœ… Enable LangSmith tracing in your LangChain environment  
- âœ… Track a simple chain execution  
- âœ… View the trace tree of a tool-using agent  
- âœ… Log multiple runs into a **Dataset**  
- âœ… Evaluate outputs with LLM-based feedback  
- âœ… Create a comparison report between prompt versions  
- âœ… Use LangSmith CLI to push data for offline testing  
- âœ… Export traces for auditing/debugging  

---

## ğŸ§¾ Cheat Sheets

### â–¶ï¸ Install LangSmith

```bash
pip install langsmith
````

### ğŸŒ Set Environment Variables

```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=sk-...
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
export LANGCHAIN_PROJECT=my-agent-project
```

Add to `.env` or shell profile for persistence.

---

### ğŸ§  Log a Chain to LangSmith

```python
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

template = "Translate the following to French: {text}"
prompt = PromptTemplate(input_variables=["text"], template=template)

chain = LLMChain(llm=OpenAI(), prompt=prompt)
result = chain.run("I love LangSmith")
```

This will auto-log into your LangSmith dashboard under the project name.

---

### ğŸ” Visual Trace Tree

* Shows each step: Prompt â†’ LLM Call â†’ Tool Use â†’ Final Output
* Drill down into input/output, tokens, model name, latency

---

### ğŸ“ Datasets

* Group of runs with inputs & outputs
* Useful for regression testing and benchmarking

```bash
langsmith dataset create "prompt-tests"
langsmith run --dataset "prompt-tests" ...
```

---

### ğŸ“Š Evaluation

```python
from langsmith.evaluation import RunEvaluator

evaluator = RunEvaluator.from_llm()
evaluator.evaluate_run(run_id="1234")
```

Or use LangSmith UI to run **LLM-based or human evaluations**.

---

## ğŸ¯ Progressive Challenges

| Level           | Challenge                                                            |
| --------------- | -------------------------------------------------------------------- |
| ğŸ¥‰ Beginner     | Enable tracing and log a simple LLMChain                             |
| ğŸ¥ˆ Intermediate | Build a Dataset and auto-evaluate 10 test prompts                    |
| ğŸ¥‡ Advanced     | Compare performance between two prompts or model versions            |
| ğŸ† Expert       | Deploy a production app with full tracing, alerting, and evaluations |

---

## ğŸ™ï¸ Interview Q\&A

* **Q:** What is LangSmith used for?
* **Q:** How does LangSmith improve debugging of LLM pipelines?
* **Q:** Can you compare two prompt versions in LangSmith?
* **Q:** What types of evaluations does LangSmith support?
* **Q:** How do you integrate LangSmith in CI/CD or a QA pipeline?

---

## ğŸ›£ï¸ Next Tech Stack Recommendations

* **LangChain** â€” LangSmith is designed for it, but can work with custom code via API
* **LLM Benchmarking Tools** â€” Like PromptLayer, Trulens, or Ragas (LangSmith > all)
* **DVC / Weights & Biases** â€” For model versioning alongside trace/version management
* **Streamlit / FastAPI** â€” Embed LangSmith-traced components in apps
* **LangGraph** â€” Build flow control logic with LangSmith as a trace and feedback layer
* **S3 / Vector DBs / Postgres** â€” Log external artifacts from LangChain chains

---

## ğŸ§¬ Tactical Philosophy

> â€œIf LangChain is how you build, **LangSmith is how you know it works.**â€

âœ… Developer observability for LLM apps
ğŸ§ª QA testing for language pipelines
ğŸ§­ Auditable, reproducible traces
âš–ï¸ Trustworthy evaluation methods
ğŸ“ˆ Actionable insights before you go to prod
