---
id: la1wcsfkgdpme31l10q1yn0
title: Models
desc: ''
updated: 1756134342881
created: 1756134336539
---

# ğŸ§° Core Machine Learning Algorithms (Detailed but digestible)

---

## ğŸ”¹ 1. **Regression Algorithms (predicting numbers)**

Goal â†’ predict **continuous values**.
Examples: house prices, temperature, stock prices.

* **Linear Regression**

  * Fits a straight line: $y = mx + c$.
  * Works if data is linearly correlated.
  * Weakness: breaks when relationships are non-linear.

* **Polynomial Regression**

  * Extends linear regression with non-linear curves.
  * Risk: easily overfits if degree is too high.

* **Regularized Regression**

  * Adds penalties to avoid overfitting.
  * **Ridge (L2 penalty)** â†’ shrinks coefficients smoothly.
  * **Lasso (L1 penalty)** â†’ can shrink some coefficients to zero (feature selection).
  * **ElasticNet** â†’ combo of both.

ğŸ‘‰ Think of regression as the â€œExcel line of best fit,â€ but smarter.

---

## ğŸ”¹ 2. **Classification Algorithms (predicting categories)**

Goal â†’ predict **discrete labels**.
Examples: spam vs. not spam, churn vs. retain, fraud vs. legit.

* **Logistic Regression**

  * Despite the name, itâ€™s for classification.
  * Outputs probabilities using the **sigmoid function**.
  * Great baseline algorithm.

* **k-Nearest Neighbors (kNN)**

  * â€œBirds of a feather flock together.â€
  * Classifies based on the majority label of nearest data points.
  * Simple, but slow on large datasets.

* **Naive Bayes**

  * Based on Bayesâ€™ Theorem.
  * â€œNaiveâ€ because it assumes features are independent.
  * Works shockingly well for **text classification** (spam filters, sentiment).

* **Support Vector Machines (SVM)**

  * Finds the **best hyperplane** to separate classes.
  * Uses **kernel trick** to handle non-linear data.
  * Powerful, but doesnâ€™t scale well to massive datasets.

---

## ğŸ”¹ 3. **Decision Trees**

Goal â†’ split data into decision rules like a flowchart.

* Intuitive and interpretable.
* Example:

  ```
  IF income > 80K AND credit_score > 700 â†’ Approve loan
  ELSE â†’ Reject
  ```
* Weakness: can overfit (memorize training data).

---

## ğŸ”¹ 4. **Ensemble Methods (crowd wisdom)**

Instead of relying on one weak model, combine many to build a strong predictor.

* **Bagging (Bootstrap Aggregating)**

  * Train multiple models on random subsets of data.
  * Average predictions (for regression) or majority vote (for classification).
  * Example: **Random Forest** (many decision trees).

* **Boosting**

  * Train models sequentially, each focusing on fixing errors of the previous.
  * Examples:

    * **AdaBoost** â†’ assigns weights to misclassified data points.
    * **Gradient Boosting Machines (GBM)** â†’ optimizes with gradient descent.
    * **XGBoost / LightGBM / CatBoost** â†’ faster, more efficient, state-of-the-art for tabular data.

* **Stacking**

  * Train multiple models and then feed their outputs into a â€œmeta-model.â€
  * Example: logistic regression combining predictions from RF + SVM + XGBoost.

ğŸ‘‰ In Kaggle competitions, **Boosting + Ensembles = gold medal recipe**.

---

## ğŸ”¹ 5. **Clustering (unsupervised)**

Goal â†’ group similar items without labels.

* **k-Means Clustering**

  * Divides data into $k$ clusters by minimizing distances to cluster centroids.
  * Simple but needs $k$ predefined.

* **Hierarchical Clustering**

  * Builds a tree of clusters (dendrogram).
  * Useful when you donâ€™t know the number of clusters.

* **DBSCAN**

  * Groups based on density.
  * Great for arbitrary shapes, and can detect noise/outliers.

---

## ğŸ”¹ 6. **Dimensionality Reduction**

When youâ€™ve got **too many features**, reduce them while preserving structure.

* **PCA (Principal Component Analysis)** â†’ projects data into fewer dimensions.
* **t-SNE / UMAP** â†’ used for visualization of high-dimensional data.

---

## ğŸ”¹ 7. **Recommendation Systems**

Special beast, but super important in industry.

* **Collaborative Filtering** â†’ â€œusers who liked X also liked Y.â€
* **Content-Based Filtering** â†’ based on item features (e.g., movie genres).
* **Hybrid Systems** â†’ combine both.

---

# âš–ï¸ When to Use What?

* Predicting numbers â†’ **Regression**
* Predicting categories â†’ **Classification**
* Finding patterns/groups â†’ **Clustering**
* Reducing noise/complexity â†’ **Dimensionality Reduction**
* Tabular data with complex patterns â†’ **Ensembles (XGBoost/LightGBM)**
* Text data â†’ **Naive Bayes, Logistic Regression, Transformers (later)**
* Images â†’ **Deep Learning (later)**

---
